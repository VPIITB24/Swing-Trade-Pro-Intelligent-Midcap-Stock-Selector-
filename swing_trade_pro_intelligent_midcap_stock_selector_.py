# -*- coding: utf-8 -*-
"""Swing Trade Pro : Intelligent Midcap Stock Selector .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19v3FtraB_4Uoqxa1v6Abut2VLG_gaqfF

# ðŸ“Š Stock Price Analysis, Screening & Forecasting â€” Project Overview

##  Objective
This project combines **technical indicator screening** with **time series forecasting** to:
1. Classify stocks into **BUY / SELL / HOLD** for swing trading.
2. Predict future stock prices  and ROI(%) using statistical and deep learning models.

---

##  Methodology

### 1. **Technical Indicator Voting System**
- **Data:** Daily Close prices for multiple tickers.
- **Indicators:** SMA(20>50), EMA(12>26), RSI, MACD, Bollinger Bands, Momentum, ATR Breakout, Stochastic, CCI.
- **Voting:** Each strategy casts a BUY or SELL vote; decisions made by threshold:
  - BUY if `BUY_Votes â‰¥ min_buy_votes`
  - SELL if `SELL_Votes â‰¥ min_sell_votes`
  - Otherwise HOLD.
- **Output:** Latest-day classification table & decision counts.

### 2. **Time Series Analysis & Forecasting**
- **Models Used:** ARIMA/SARIMA, Facebook Prophet, LSTM.
- **Steps:**
  - Stationarity check (ADF test).
  - Trend/seasonality analysis.
  - Model training & validation.
  - Next N-day forecast generation.
- **Evaluation Metrics:** RMSE, MAPE, MAE.
- **Output:** Historical vs predicted price charts.

---

## Key Insights
- Ensemble voting from diverse indicators reduces false signals.
- ARIMA works well for short-term linear patterns.
- Prophet captures seasonal effects effectively.
- LSTM handles nonlinear relationships in price data.
- Combining forecasts with indicator signals could improve swing trade timing.

---

## Outputs
- **Tables:** `voting_summary.csv` / `.xlsx` with votes, counts, and decisions.
- **Forecast Plots:** Side-by-side actual vs predicted prices.
- **Metrics:** Accuracy scores for each forecasting model.
"""

# Import necessary libraries for the project
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from prophet import Prophet
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from scipy.stats import ttest_ind, f_oneway
from tensorflow.keras.callbacks import EarlyStopping
from math import sqrt
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

warnings.filterwarnings('ignore')

Stockname = [
    "ACMESOLAR.NS", "AADHARHFC.NS", "AARTIIND.NS", "AAVAS.NS", "ACE.NS",
    "ABREL.NS", "ABSLAMC.NS", "AEGISLOG.NS", "AFCONS.NS", "AFFLE.NS",
    "AKUMS.NS", "APLLTD.NS", "ALIVUS.NS", "ALKYLAMINE.NS", "ALOKINDS.NS",
    "ARE&M.NS", "AMBER.NS", "ANANDRATHI.NS", "ANANTRAJ.NS", "ANGELONE.NS",
    "APTUS.NS", "ASAHIINDIA.NS", "ASTERDM.NS", "ASTRAZEN.NS", "ATUL.NS",
    "AIIL.NS", "BASF.NS", "BEML.NS", "BLS.NS", "BALRAMCHIN.NS",
    "BATAINDIA.NS", "BAYERCROP.NS", "BIKAJI.NS", "BSOFT.NS", "BLUEDART.NS",
    "BBTC.NS", "FIRSTCRY.NS", "BRIGADE.NS", "MAPMYINDIA.NS", "CCL.NS",
    "CESC.NS", "CAMPUS.NS", "CANFINHOME.NS", "CAPLIPOINT.NS", "CGCL.NS",
    "CARBORUNIV.NS", "CASTROLIND.NS", "CEATLTD.NS", "CENTRALBK.NS", "CDSL.NS",
    "CENTURYPLY.NS", "CERA.NS", "CHALET.NS", "CHAMBLFERT.NS", "CHENNPETRO.NS",
    "CHOLAHLDNG.NS", "CUB.NS", "CLEAN.NS", "CAMS.NS", "CONCORDBIO.NS",
    "CRAFTSMAN.NS", "CREDITACC.NS", "CROMPTON.NS", "CYIENT.NS", "DCMSHRIRAM.NS",
    "DOMS.NS", "DATAPATTNS.NS", "DEEPAKFERT.NS", "DELHIVERY.NS", "DEVYANI.NS",
    "LALPATHLAB.NS", "EIDPARRY.NS", "EIHOTEL.NS", "ELECON.NS", "ELGIEQUIP.NS",
    "EMCURE.NS", "ENGINERSIN.NS", "ERIS.NS", "FACT.NS", "FINCABLES.NS",
    "FINPIPE.NS", "FSL.NS", "FIVESTAR.NS", "GRSE.NS", "GILLETTE.NS",
    "GODIGIT.NS", "GPIL.NS", "GODFRYPHLP.NS", "GODREJAGRO.NS", "GRANULES.NS",
    "GRAPHITE.NS", "GRAVITA.NS", "GESHIP.NS", "GMDCLTD.NS", "GNFC.NS",
    "GPPL.NS", "GSPL.NS", "HEG.NS", "HBLENGINE.NS", "HFCL.NS",
    "HAPPSTMNDS.NS", "HSCL.NS", "HINDCOPPER.NS", "HOMEFIRST.NS", "HONASA.NS",
    "IDBI.NS", "IFCI.NS", "IIFL.NS", "INOXINDIA.NS", "IRCON.NS",
    "ITI.NS", "INDGN.NS", "INDIACEM.NS", "INDIAMART.NS", "IEX.NS",
    "IOB.NS", "INOXWIND.NS", "INTELLECT.NS", "IGIL.NS", "IKS.NS",
    "JBCHEPHARM.NS", "JBMA.NS", "JKTYRE.NS", "JMFINANCIL.NS", "JSWHL.NS",
    "JPPOWER.NS", "J&KBANK.NS", "JINDALSAW.NS", "JUBLINGREA.NS", "JUBLPHARMA.NS",
    "JWL.NS", "JUSTDIAL.NS", "JYOTHYLAB.NS", "JYOTICNC.NS", "KNRCON.NS",
    "KAJARIACER.NS", "KPIL.NS", "KANSAINER.NS", "KARURVYSYA.NS", "KAYNES.NS",
    "KEC.NS", "KFINTECH.NS", "KIRLOSBROS.NS", "KIRLOSENG.NS", "KIMS.NS",
    "LTFOODS.NS", "LATENTVIEW.NS", "LAURUSLABS.NS", "LEMONTREE.NS", "MMTC.NS",
    "MGL.NS", "MAHSEAMLES.NS", "MANAPPURAM.NS", "MASTEK.NS", "METROPOLIS.NS",
    "MINDACORP.NS", "MCX.NS", "NATCOPHARM.NS", "NBCC.NS", "NCC.NS",
    "NSLNISP.NS", "NH.NS", "NAVA.NS", "NAVINFLUOR.NS", "NETWEB.NS",
    "NETWORK18.NS", "NEULANDLAB.NS", "NEWGEN.NS", "NIVABUPA.NS", "NUVAMA.NS",
    "OLECTRA.NS", "PCBL.NS", "PGEL.NS", "PNBHOUSING.NS", "PNCINFRA.NS",
    "PTCIL.NS", "PVRINOX.NS", "PFIZER.NS", "PEL.NS", "PPLPHARMA.NS",
    "POLYMED.NS", "POONAWALLA.NS", "PRAJIND.NS", "RRKABEL.NS", "RBLBANK.NS",
    "RHIM.NS", "RITES.NS", "RADICO.NS", "RAILTEL.NS", "RAINBOW.NS",
    "RKFORGE.NS", "RCF.NS", "RTNINDIA.NS", "RAYMONDLSL.NS", "RAYMOND.NS",
    "REDINGTON.NS", "RPOWER.NS", "ROUTE.NS", "SBFC.NS", "SKFINDIA.NS",
    "SAGILITY.NS", "SAILIFE.NS", "SAMMAANCAP.NS", "SAPPHIRE.NS", "SARDAEN.NS",
    "SAREGAMA.NS", "SCHNEIDER.NS", "SCI.NS", "RENUKA.NS", "SHYAMMETL.NS",
    "SIGNATURE.NS", "SOBHA.NS", "SONATSOFTW.NS", "SWSOLAR.NS", "SUMICHEM.NS",
    "SUNDRMFAST.NS", "SWANENERGY.NS", "SYRMA.NS", "TBOTEK.NS", "TANLA.NS",
    "TATACHEM.NS", "TTML.NS", "TECHNOE.NS", "TEJASNET.NS", "RAMCOCEM.NS",
    "TIMKEN.NS", "TITAGARH.NS", "TARIL.NS", "TRIDENT.NS", "TRIVENI.NS",
    "TRITURBINE.NS", "UCOBANK.NS", "UTIAMC.NS", "USHAMART.NS", "VGUARD.NS",
    "DBREALTY.NS", "VTL.NS", "MANYAVAR.NS", "VIJAYA.NS", "WELCORP.NS",
    "WELSPUNLIV.NS", "WESTLIFE.NS", "WHIRLPOOL.NS", "WOCKPHARMA.NS", "ZFCVINDIA.NS","ZEEL.NS", "ZENTEC.NS", "ZENSARTECH.NS", "ECLERX.NS"

]

import yfinance as yf

start ="2023-01-01"
end =  "2025-07-06"

ticker = Stockname

# Get data from January 1, 2023 to the present
df1 = yf.download(ticker, start= start,end= end)

# print the df1
df1.head()

# shape of the df1
df1.shape

# store   only close price data  of each stocks :
df= df1['Close']

df.head()

# =========================
# STRATEGY DEFINITIONS (BUY/SELL)
# =========================

def strategy_sma_cross(close):
    sma9 = close.rolling(window=9).mean()
    sma20 = close.rolling(window=20).mean()
    return np.where(sma9 > sma20, "BUY", "SELL")

def strategy_ema_cross(close):
    ema12 = close.ewm(span=12, adjust=False).mean()
    ema26 = close.ewm(span=26, adjust=False).mean()
    return np.where(ema12 > ema26, "BUY", "SELL")

def strategy_rsi(close):
    delta = close.diff()
    gain = delta.where(delta > 0, 0).rolling(window=14).mean()
    loss = -delta.where(delta < 0, 0).rolling(window=14).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return np.where(rsi < 30, "BUY", "SELL")

def strategy_macd(close):
    ema12 = close.ewm(span=12, adjust=False).mean()
    ema26 = close.ewm(span=26, adjust=False).mean()
    macd = ema12 - ema26
    signal = macd.ewm(span=9, adjust=False).mean()
    return np.where(macd > signal, "BUY", "SELL")

def strategy_bollinger(close):
    sma20 = close.rolling(window=20).mean()
    stddev = close.rolling(window=20).std()
    lower_band = sma20 - (2 * stddev)
    return np.where(close <= lower_band, "BUY", "SELL")

def strategy_momentum(close):
    momentum = close - close.shift(10)
    return np.where(momentum > 0, "BUY", "SELL")

def strategy_atr_breakout(close):
    tr = (close - close.shift(1)).abs()
    atr = tr.rolling(window=14).mean()
    upper_break = close.shift(1) + (2 * atr)
    return np.where(close > upper_break, "BUY", "SELL")

def strategy_stochastic(close, k_window=14, d_window=3):
    low_min = close.rolling(window=k_window).min()
    high_max = close.rolling(window=k_window).max()
    k_percent = 100 * (close - low_min) / (high_max - low_min)
    return np.where(k_percent < 25, "BUY", "SELL")

def strategy_cci(close, n=20):
    sma = close.rolling(window=n).mean()
    mad = close.rolling(window=n).apply(lambda x: np.mean(np.abs(x - np.mean(x))), raw=True)
    cci = (close - sma) / (0.015 * mad)
    return np.where(cci < -80, "BUY", "SELL")

# =========================
# STRATEGY LIST
# =========================
strategies = [
    strategy_sma_cross,
    strategy_ema_cross,
    strategy_rsi,
    strategy_macd,
    strategy_bollinger,
    strategy_momentum,
    strategy_atr_breakout,
    strategy_stochastic,
    strategy_cci
]

# =========================
# VOTING SYSTEM (BUY/SELL/HOLD)
# =========================
from collections import Counter

buy_stocks = []
sell_stocks = []
hold_stocks = []

# ---- Manual condition settings ----
min_buy_votes = 8
min_sell_votes = 8

for ticker in df.columns:
    close_prices = df[ticker]
    votes = []

    for strat in strategies:
        signals = strat(close_prices)
        votes.append(signals[-1])  # last day's vote

    vote_counts = Counter(votes)
    buy_votes = vote_counts.get("BUY", 0)
    sell_votes = vote_counts.get("SELL", 0)

     # ---- Apply manual conditions ----
    if buy_votes >= min_buy_votes:
        buy_stocks.append(ticker)
    elif sell_votes >= min_sell_votes:
        sell_stocks.append(ticker)
    else:
        hold_stocks.append(ticker)

# =========================
# RESULTS
# =========================
print("BUY stocks:", buy_stocks)
print("SELL stocks:", sell_stocks)
print("HOLD stocks:", hold_stocks)

# Now Start the main project

# download the stocks data for a perticular stock
import yfinance as yf
stock = ['CERA.NS']

# Get data
df2 = yf.download(stock, start= start, end= end)

df2.head()

# rename columns
df2 = df2.reset_index()
df2.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']
print(df2.head())

df2.columns

# EDA: Check for missing values

print("Missing values:\n", df2.isnull().sum())

# Descriptive statistics
df2.describe()

# Seeing the data type
df2.info()

# Create candlestick chart
fig = go.Figure
fig = go.Figure(data=[
    go.Candlestick(
        x=df2['Date'],
        open=df2['Open'],
        high=df2['High'],
        low=df2['Low'],
        close=df2['Close'],
        increasing_line_color='green',
        increasing_fillcolor='green',
        decreasing_line_color='red',
        decreasing_fillcolor='red',
        line_width=1,           # Wick thickness
        whiskerwidth=0.8
    )
])

# Style the layout

fig.update_layout(
    title=f'{stock} Candlestick Chart',
    title_x=0.5,
    xaxis_title='Date',
    yaxis_title='Price',
    template='plotly_dark',
    xaxis_tickangle=-45,
    xaxis_rangeslider_visible=False,
    width=1250,
    height=600
)

fig.show()

# Date vs Open Plot

fig = px.line(df2, x='Date', y='Open', title=f'{stock} Open Price vs Time')
fig.update_layout(
    title_x=0.5,
    xaxis_title='Date',
    yaxis_title='Open Price',
    template='plotly_dark',
    xaxis_tickformat='%d %B, %Y',
    xaxis_tickangle=-45
)
fig.show()

# Date vs Close price Plot

fig = px.line(df2, x='Date', y='Close', title=f'{stock}Closing Price Over Time')
fig.update_layout(
    title_x=0.5,
    xaxis_title='Date',
    yaxis_title='Close Price',
    template='plotly_dark',
    xaxis_tickformat='%d %B, %Y',
    xaxis_tickangle=-45
)
fig.show()

#  Distribution Plot of the all features

fig = make_subplots(rows=3, cols=2, subplot_titles=('Close', 'Volume', 'Open', 'High', 'Low'))
fig.add_trace(go.Histogram(x=df2['Close'], name='Close'), row=1, col=1)
fig.add_trace(go.Histogram(x=df2['Volume'], name='Volume'), row=1, col=2)
fig.add_trace(go.Histogram(x=df2['Open'], name='Open'), row=2, col=1)
fig.add_trace(go.Histogram(x=df2['High'], name='High'), row=2, col=2)
fig.add_trace(go.Histogram(x=df2['Low'], name='Low'), row=3, col=1)
fig.update_layout(title_text='Distribution of Features', title_x=0.5, template='plotly_dark')
fig.show()

# Relationship  with eachother in features
fig = px.scatter_matrix(df2, dimensions=['Open', 'High', 'Low', 'Close', 'Close', 'Volume'], title='Scatter Matrix')
fig.update_layout(title_text='Scatter Matrix', title_x=0.5, template='plotly_dark')
fig.show()

# Correlation plot
import plotly.express as px
correlation = df2.select_dtypes(include=[np.number]).corr().round(2)
fig = px.imshow(
    correlation,
    text_auto=True,
    color_continuous_scale='RdBu_r',
    aspect='auto',
    title='Correlation Matrix'
)
fig.update_layout(
    title_x=0.5,
    template='plotly_dark',
    margin=dict(l=60, r=60, t=80, b=60)
)
fig.show()

# Statistical Tests
# T-test comparing 'High' and 'Low' prices
t_stat, p_value = ttest_ind(df2['High'], df2['Low'])
t_test_result = {
    'Statistic': [t_stat],
    'p-value': [p_value]
}
t_test_df = pd.DataFrame(t_test_result)
t_test_df

# ANOVA test for 'Open', 'High', 'Low', 'Close' prices
anova_stat, anova_p_value = f_oneway(df2['Open'], df2['High'], df2['Low'], df2['Close'])
anova_result = {
    'Statistic': [anova_stat],
    'p-value': [anova_p_value]
}
anova_df = pd.DataFrame(anova_result)
anova_df

from statsmodels.tsa.stattools import adfuller, kpss
from scipy.stats import kstest, norm
import pandas as pd

def check_stationarity_tests(df2, col):
    series = df2[col].dropna()

    # ADF Test
    adf_stat, adf_p, _, _, adf_crit, _ = adfuller(series)
    adf_result = {
        "Test": "ADF",
        "Test Statistic": round(adf_stat, 4),
        "p-value": round(adf_p, 4),
        "Conclusion": "Stationary" if adf_p < 0.05 else "Non-Stationary"
    }

    # KPSS Test
    try:
        kpss_stat, kpss_p, _, kpss_crit = kpss(series, regression='c', nlags='auto')
        kpss_result = {
            "Test": "KPSS",
            "Test Statistic": round(kpss_stat, 4),
            "p-value": round(kpss_p, 4),
            "Conclusion": "Stationary" if kpss_p > 0.05 else "Non-Stationary"
        }
    except Exception as e:
        kpss_result = {
            "Test": "KPSS",
            "Test Statistic": None,
            "p-value": None,
            "Conclusion": f"Error: {str(e)}"
        }

    # KS Test on returns (for normality)
    returns = series.pct_change().dropna()
    ks_stat, ks_p = kstest(returns, 'norm', args=(returns.mean(), returns.std()))
    ks_result = {
        "Test": "KS",
        "Test Statistic": round(ks_stat, 4),
        "p-value": round(ks_p, 4),
        "Conclusion": "Normal" if ks_p > 0.05 else "Not Normal"
    }

    return pd.DataFrame([adf_result, kpss_result, ks_result])
check_stationarity_tests(df2, 'Close')

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Drop NaNs
series = df2['Close'].dropna()

# Create full-page layout
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

# Plot ACF
plot_acf(series, lags=50, ax=axes[0])
axes[0].set_title('Autocorrelation Function (ACF)', fontsize=14)

# Plot PACF
plot_pacf(series, lags=50, ax=axes[1], method='ywm')  # Use yule-walker to avoid warnings
axes[1].set_title('Partial Autocorrelation Function (PACF)', fontsize=14)

plt.tight_layout()
plt.show()

# make stationary:
df2['diff_close'] = df2['Close'].diff()
df2['seasonal_diff'] = df2['diff_close'] - df2['diff_close'].shift(5)
df2 = df2.dropna()

from statsmodels.tsa.stattools import adfuller
adfuller(df2['diff_close'])

# cheak again stationary

def check_stationarity_tests(df2, col):
    series = df2[col].dropna()

    # ADF Test
    adf_stat, adf_p, _, _, adf_crit, _ = adfuller(series)
    adf_result = {
        "Test": "ADF",
        "Test Statistic": round(adf_stat, 4),
        "p-value": round(adf_p, 4),
        "Conclusion": "Stationary" if adf_p < 0.05 else "Non-Stationary"
    }

    # KPSS Test
    try:
        kpss_stat, kpss_p, _, kpss_crit = kpss(series, regression='c', nlags='auto')
        kpss_result = {
            "Test": "KPSS",
            "Test Statistic": round(kpss_stat, 4),
            "p-value": round(kpss_p, 4),
            "Conclusion": "Stationary" if kpss_p > 0.05 else "Non-Stationary"
        }
    except Exception as e:
        kpss_result = {
            "Test": "KPSS",
            "Test Statistic": None,
            "p-value": None,
            "Conclusion": f"Error: {str(e)}"
        }

    # KS Test on returns (for normality)
    returns = series.pct_change().dropna()
    ks_stat, ks_p = kstest(returns, 'norm', args=(returns.mean(), returns.std()))
    ks_result = {
        "Test": "KS",
        "Test Statistic": round(ks_stat, 4),
        "p-value": round(ks_p, 4),
        "Conclusion": "Normal" if ks_p > 0.05 else "Not Normal"
    }

    return pd.DataFrame([adf_result, kpss_result, ks_result])
check_stationarity_tests(df2, 'seasonal_diff')

# STL Plot for open price

# STL decomposition
stl = STL(df2['Open'], period=30)
res = stl.fit()

# Extract components
observed = df2['Open']
trend = res.trend
seasonal = res.seasonal
resid = res.resid
dates = df2['Date']

# Create 4-row subplot
fig = make_subplots(
    rows=4, cols=1,
    shared_xaxes=True,
    vertical_spacing=0.03,
    subplot_titles=("Observed", "Trend", "Seasonal", "Residual")
)

fig.add_trace(go.Scatter(x=dates, y=observed, name='Observed', line=dict(color='white')), row=1, col=1)
fig.add_trace(go.Scatter(x=dates, y=trend, name='Trend', line=dict(color='orange')), row=2, col=1)
fig.add_trace(go.Scatter(x=dates, y=seasonal, name='Seasonal', line=dict(color='cyan')), row=3, col=1)
fig.add_trace(go.Scatter(x=dates, y=resid, name='Residual', line=dict(color='magenta')), row=4, col=1)

# Layout
fig.update_layout(
    height=900,
    title_text="ðŸ“ˆ STL Decomposition of Open Price",
    template='plotly_dark',
    showlegend=False,
    title_x=0.5
)

fig.show()

# STL Plot for open price

# STL decomposition
stl = STL(df2['Close'], period=30)
res = stl.fit()

# Create subplots
from plotly.subplots import make_subplots

fig = make_subplots(
    rows=4, cols=1,
    shared_xaxes=True,
    vertical_spacing=0.03,
    subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residual')
)

fig.add_trace(go.Scatter(y=res.observed, mode='lines', name='Observed', line=dict(color='cyan')), row=1, col=1)
fig.add_trace(go.Scatter(y=res.trend, mode='lines', name='Trend', line=dict(color='orange')), row=2, col=1)
fig.add_trace(go.Scatter(y=res.seasonal, mode='lines', name='Seasonal', line=dict(color='lime')), row=3, col=1)
fig.add_trace(go.Scatter(y=res.resid, mode='lines', name='Residual', line=dict(color='magenta')), row=4, col=1)

fig.update_layout(
    height=900,
    template='plotly_dark',
    title_text='STL Decomposition of Close Price',
    title_x=0.5,
    showlegend=False
)

fig.show()

# see the seanality strength on defferent values period
for p in [7, 15, 30,45,60,90]:
    stl = STL(df2['seasonal_diff'], period=p)
    res = stl.fit()
    s_strength = 1 - res.resid.var() / (res.resid.var() + res.seasonal.var())
    print(f"Period {p} => Seasonal Strength: {s_strength:.2f}")

# set the index
df2 = df2.set_index('Date')
df2 = df2.reset_index()

df2

# plot box plot for see the close price by open the day of market
import plotly.express as px

# Add day names column
df2['DayOfWeek'] = df2['Date'].dt.day_name()

# Define weekday order
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# Plot
fig = px.box(
    df2, x='DayOfWeek', y='Close',
    category_orders={'DayOfWeek': day_order},
    title='ðŸ“Š Stock Close Price Distribution by Day of Week',
    template='plotly_dark',
    color='DayOfWeek'
)

fig.update_layout(
    title_x=0.5,
    xaxis_title='Day of Week',
    yaxis_title='Close Price',
    showlegend=False
)

fig.show()

# Trendâ€“Momentumâ€“Timing confirmation Plot

from plotly.subplots import make_subplots

# --- In-Memory Indicator Calculation ---

# EMA
ema20 = df2['Close'].ewm(span=20).mean()
ema50 = df2['Close'].ewm(span=50).mean()

# MACD
ema12 = df2['Close'].ewm(span=12).mean()
ema26 = df2['Close'].ewm(span=26).mean()
macd = ema12 - ema26
signal = macd.ewm(span=9).mean()
macd_hist = macd - signal

# RSI
delta = df2['Close'].diff()
gain = delta.clip(lower=0)
loss = -delta.clip(upper=0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
rsi = 100 - (100 / (1 + rs))

# --- Plotting ---
fig = make_subplots(
    rows=3, cols=1, shared_xaxes=True,
    row_heights=[0.5, 0.25, 0.25],
    subplot_titles=['Candlestick + EMA', 'MACD', 'RSI']
)

# Candlestick + EMA (Row 1)
fig.add_trace(go.Candlestick(
    x=df2['Date'], open=df2['Open'], high=df2['High'],
    low=df2['Low'], close=df2['Close'],
    increasing_line_color='lime', decreasing_line_color='red',
    name='Candles'
), row=1, col=1)

fig.add_trace(go.Scatter(x=df2['Date'], y=ema20, line=dict(color='orange'), name='EMA20'), row=1, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=ema50, line=dict(color='cyan'), name='EMA50'), row=1, col=1)

# MACD (Row 2)
fig.add_trace(go.Scatter(x=df2['Date'], y=macd, line=dict(color='white'), name='MACD'), row=2, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=signal, line=dict(color='orange'), name='Signal'), row=2, col=1)
fig.add_trace(go.Bar(x=df2['Date'], y=macd_hist, marker_color='lightblue', name='Histogram'), row=2, col=1)

# RSI (Row 3)
fig.add_trace(go.Scatter(x=df2['Date'], y=rsi, line=dict(color='magenta'), name='RSI'), row=3, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=[70]*len(df), line=dict(color='red', dash='dash'), showlegend=False), row=3, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=[30]*len(df), line=dict(color='green', dash='dash'), showlegend=False), row=3, col=1)

# Layout
fig.update_layout(
    template='plotly_dark',
    height=1000,
    title= "Trendâ€“Momentumâ€“Timing confirmation system",
    xaxis_rangeslider_visible=False,
    hovermode='x unified'
)

fig.show()

# Bollingerâ€“RSIâ€“Volume Confirmation Plot

# --- In-Memory Bollinger Bands Calculation ---
ma20 = df2['Close'].rolling(window=20).mean()
std20 = df2['Close'].rolling(window=20).std()
bb_upper = ma20 + 2 * std20
bb_lower = ma20 - 2 * std20

# --- In-Memory RSI Calculation ---
delta = df2['Close'].diff()
gain = delta.clip(lower=0)
loss = -delta.clip(upper=0)
avg_gain = gain.rolling(window=14).mean()
avg_loss = loss.rolling(window=14).mean()
rs = avg_gain / avg_loss
rsi = 100 - (100 / (1 + rs))

# --- Plotting ---
fig = make_subplots(
    rows=3, cols=1, shared_xaxes=True,
    row_heights=[0.5, 0.2, 0.3],
    subplot_titles=['Candlestick + Bollinger Bands', 'Volume', 'RSI']
)

# Candlestick + Bollinger Bands (Row 1)
fig.add_trace(go.Candlestick(
    x=df2['Date'], open=df2['Open'], high=df2['High'],
    low=df2['Low'], close=df2['Close'],
    increasing_line_color='lime', decreasing_line_color='red',
    name='Candles'
), row=1, col=1)

fig.add_trace(go.Scatter(x=df2['Date'], y=bb_upper, line=dict(color='red', width=1), name='Upper Band'), row=1, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=bb_lower, line=dict(color='blue', width=1), name='Lower Band'), row=1, col=1)

# Volume (Row 2)
fig.add_trace(go.Bar(
    x=df2['Date'], y=df2['Volume'],
    marker_color='lightgreen',
    name='Volume'
), row=2, col=1)

# RSI (Row 3)
fig.add_trace(go.Scatter(x=df2['Date'], y=rsi, line=dict(color='magenta'), name='RSI'), row=3, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=[70]*len(df2), line=dict(color='red', dash='dash'), showlegend=False), row=3, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=[30]*len(df2), line=dict(color='green', dash='dash'), showlegend=False), row=3, col=1)

# Layout
fig.update_layout(
    template='plotly_dark',
    height=900,
    title='Bollingerâ€“RSIâ€“Volume Confirmation',
    xaxis_rangeslider_visible=False,
    hovermode='x unified',
    font=dict(color='white')
)

fig.show()

# Trendâ€“Volatilityâ€“Momentum  Plot

# --- In-memory Calculations ---
# EMA
ema20 = df2['Close'].ewm(span=20).mean()
ema50 = df2['Close'].ewm(span=50).mean()

# Bollinger Bands
ma20 = df2['Close'].rolling(window=20).mean()
std20 = df2['Close'].rolling(window=20).std()
bb_upper = ma20 + 2 * std20
bb_lower = ma20 - 2 * std20

# MACD
ema12 = df2['Close'].ewm(span=12, adjust=False).mean()
ema26 = df2['Close'].ewm(span=26, adjust=False).mean()
macd = ema12 - ema26
signal = macd.ewm(span=9, adjust=False).mean()
macd_hist = macd - signal

# --- Plotting ---
fig = make_subplots(
    rows=2, cols=1, shared_xaxes=True,
    row_heights=[0.6, 0.4],
    subplot_titles=['Candlestick + EMA + Bollinger Bands', 'MACD']
)

# Candlestick + EMA + BB
fig.add_trace(go.Candlestick(
    x=df2['Date'], open=df2['Open'], high=df2['High'],
    low=df2['Low'], close=df2['Close'],
    increasing_line_color='lime', decreasing_line_color='red',
    name='Candles'
), row=1, col=1)

fig.add_trace(go.Scatter(x=df2['Date'], y=ema20, line=dict(color='orange'), name='EMA20'), row=1, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=ema50, line=dict(color='cyan'), name='EMA50'), row=1, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=bb_upper, line=dict(color='red', width=1), name='Upper BB'), row=1, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=bb_lower, line=dict(color='blue', width=1), name='Lower BB'), row=1, col=1)

# MACD
fig.add_trace(go.Scatter(x=df2['Date'], y=macd, line=dict(color='white'), name='MACD'), row=2, col=1)
fig.add_trace(go.Scatter(x=df2['Date'], y=signal, line=dict(color='orange'), name='Signal'), row=2, col=1)
fig.add_trace(go.Bar(x=df2['Date'], y=macd_hist, marker_color='lightblue', name='Histogram'), row=2, col=1)

# Layout
fig.update_layout(
    template='plotly_dark',
    height=800,
    title='Trendâ€“Volatilityâ€“Momentum ',
    xaxis_rangeslider_visible=False,
    hovermode='x unified',
    font=dict(color='white')
)
fig.show()

# corr. matrix plot
correlation = df2.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Stocks")
plt.tight_layout()
plt.show()

## Arima model fit

import matplotlib.pyplot as plt

# Train-test split (80% train, 20% test) on Close price
train_data = df2['Close'].iloc[:int(len(df2) * 0.8)]
test_data = df2['Close'].iloc[int(len(df2) * 0.8):]

# Plot
plt.figure(figsize=(12, 7))
plt.title(f'{stock[0]} Close Prices')
plt.xlabel('Dates')
plt.ylabel('Close Price')
plt.plot(train_data.index, train_data, color='blue', label='Training Data')
plt.plot(test_data.index, test_data, color='green', label='Testing Data')
plt.legend()
plt.show()

df2['Date'] = pd.to_datetime(df2['Date'])  # convert date column to datetime
df2.set_index('Date', inplace=True)       # set as datetime index

n_days_forecast = 10      # Forecast horizon
order = (1, 1, 1)         # ARIMA order (p, d, q)
train_ratio = 0.8         # % of data for training

# -----------------------------
# Train-Test Split for Backtesting
# -----------------------------
train_size = int(len(df2) * train_ratio)
train_data = df2['Close'][:train_size]
test_data = df2['Close'][train_size:]

# -----------------------------
# Walk-forward backtesting for ARIMA
# -----------------------------
history = list(train_data.values)
predictions = []

for t in range(len(test_data)):
    # Fit ARIMA model on history
    model = ARIMA(history, order=order)
    model_fit = model.fit()

    # Forecast next value
    yhat = model_fit.forecast()[0]
    predictions.append(yhat)

    # Append actual observation to history
    history.append(test_data.iloc[t])

# -----------------------------
# Backtesting metrics
# -----------------------------
mse = mean_squared_error(test_data, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(test_data, predictions)
mape = np.mean(np.abs((test_data - predictions)/test_data)) * 100
smape = 100 * np.mean(2 * np.abs(test_data - predictions) / (np.abs(test_data) + np.abs(predictions)))
r2 = r2_score(test_data, predictions)

# Direction accuracy
direction_true = np.sign(np.diff(test_data))
direction_pred = np.sign(np.diff(predictions))
hit_rate = np.mean(direction_true == direction_pred) * 100

print(f"MSE: {mse:.3f}, RMSE: {rmse:.3f}, MAE: {mae:.3f}")
print(f"MAPE: {mape:.2f}%, SMAPE: {smape:.2f}%, RÂ²: {r2:.3f}")
print(f"Direction accuracy: {hit_rate:.2f}%")

# Plot backtesting results
# -----------------------------
plt.figure(figsize=(12, 7))
plt.plot(train_data.index, train_data, label='Training Data', color='blue')
plt.plot(test_data.index, test_data, label='Actual Test Data', color='red')
plt.plot(test_data.index, predictions, label='Predicted Test Data', color='green', linestyle='dashed')
plt.title(f"{stock[0]} Backtest (ARIMA{order})")
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# -----------------------------
# Final Forecast (n days ahead)
# -----------------------------
model_final = ARIMA(df2['Close'].values, order=order)
model_final_fit = model_final.fit()

forecast_obj = model_final_fit.get_forecast(steps=n_days_forecast)
forecast_mean = forecast_obj.predicted_mean
conf_int = forecast_obj.conf_int(alpha=0.05)

# Future business days
last_date = df2.index[-1]
future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_days_forecast, freq='B')

forecast_df2 = pd.DataFrame({
    'Date': future_dates,
    'Forecasted_Close': forecast_mean,
    'Lower_Bound': conf_int[:, 0],
    'Upper_Bound': conf_int[:, 1]
}).set_index('Date')

print("\nForecast:")
print(forecast_df2)

# -----------------------------
# Plot forecast
# -----------------------------
plt.figure(figsize=(12, 7))
plt.plot(df2.index, df2['Close'], color='blue', label='Actual Close Price')
plt.plot(forecast_df2.index, forecast_df2['Forecasted_Close'], color='green', marker='o', linestyle='dashed', label='Forecasted Price')
plt.fill_between(forecast_df2.index, forecast_df2['Lower_Bound'], forecast_df2['Upper_Bound'], color='gray', alpha=0.3, label='95% Confidence Interval')
plt.title(f'{stock[0]} Price Forecast ({n_days_forecast} Days Ahead)')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# 1. Calculate daily returns (%) from forecasted prices
forecast_df2['Daily_Return_%'] = forecast_df2['Forecasted_Close'].pct_change().fillna(0) * 100

forecast_df2['Cumulative_ROI_%'] = (forecast_df2['Forecasted_Close'] / forecast_df2['Forecasted_Close'].iloc[0] - 1) * 100

# 3. Plot cumulative ROI over forecast horizon
plt.figure(figsize=(12, 7))
plt.plot(forecast_df2.index, forecast_df2['Cumulative_ROI_%'], marker='o', color='purple', label='Cumulative ROI (%)')
plt.title(f'Cumulative Return on Investment (ROI) for {n_days_forecast}-Day Forecast')
plt.xlabel('Date')
plt.ylabel('ROI (%)')
plt.grid(True)
plt.legend()
plt.show()

#SARIMA:

# -----------------------------
# Parameters
# -----------------------------
n_days_forecast = 10        # Forecast horizon
order = (2, 1, 2)           # (p,d,q) for ARIMA part
seasonal_order = (1, 1, 1, 5)  # (P,D,Q,s) for seasonal part
train_ratio = 0.8           # Train-test split ratio

# -----------------------------
# Train-Test Split for Backtesting
# -----------------------------
train_size = int(len(df2) * train_ratio)
train_data = df2['Close'][:train_size]
test_data = df2['Close'][train_size:]

history = list(train_data.values)
predictions = []

for t in range(len(test_data)):
    model = SARIMAX(history, order=order, seasonal_order=seasonal_order,
                    enforce_stationarity=False, enforce_invertibility=False)
    model_fit = model.fit(disp=False)

    yhat = model_fit.forecast(steps=1)[0]
    predictions.append(yhat)
    history.append(test_data.iloc[t])

# Metrics
mse = mean_squared_error(test_data, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(test_data, predictions)
mape = np.mean(np.abs((test_data - predictions)/test_data)) * 100
smape = 100 * np.mean(2 * np.abs(test_data - predictions) / (np.abs(test_data) + np.abs(predictions)))
r2 = r2_score(test_data, predictions)

# Direction accuracy
direction_true = np.sign(np.diff(test_data))
direction_pred = np.sign(np.diff(predictions))
hit_rate = np.mean(direction_true == direction_pred) * 100

print(f"MSE: {mse:.3f}, RMSE: {rmse:.3f}, MAE: {mae:.3f}")
print(f"MAPE: {mape:.2f}%, SMAPE: {smape:.2f}%, RÂ²: {r2:.3f}")
print(f"Direction accuracy: {hit_rate:.2f}%")

# -----------------------------
# Plot backtesting results
# -----------------------------
plt.figure(figsize=(12, 7))
plt.plot(train_data.index, train_data, label='Training Data', color='blue')
plt.plot(test_data.index, test_data, label='Actual Test Data', color='red')
plt.plot(test_data.index, predictions, label='Predicted Test Data', color='green', linestyle='dashed')
plt.title(f"{stock[0]} Backtest (SARIMA{order}x{seasonal_order})")
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# Plot forecast
# -----------------------------
plt.figure(figsize=(12, 7))
plt.plot(df2.index, df2['Close'], color='blue', label='Actual Close Price')
plt.plot(forecast_df2.index, forecast_df2['Forecasted_Close'], color='green', marker='o', linestyle='dashed', label='Forecasted Price')
plt.fill_between(forecast_df2.index, forecast_df2['Lower_Bound'], forecast_df2['Upper_Bound'], color='gray', alpha=0.3, label='95% Confidence Interval')
plt.title(f'{stock[0]} Price Forecast ({n_days_forecast} Days Ahead) - SARIMA')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Calculate daily returns
forecast_df2['Daily_Return'] = forecast_df2['Forecasted_Close'].pct_change()

# Calculate cumulative ROI (%)
forecast_df2['Cumulative_ROI_%'] = ((1 + forecast_df2['Daily_Return']).cumprod() - 1) * 100

# Fill NaN for clean plotting
forecast_df2['Daily_Return'].fillna(0, inplace=True)

# Plot Cumulative ROI (%) over forecast horizon
plt.figure(figsize=(12, 7))
plt.plot(forecast_df2.index, forecast_df2['Cumulative_ROI_%'],
         marker='o', color='purple', label='Cumulative ROI (%)')

plt.title(f'Cumulative Return on Investment (ROI) for {n_days_forecast}-Day SARIMAX Forecast')
plt.xlabel('Date')
plt.ylabel('Cumulative ROI (%)')
plt.grid(True)
plt.legend()
plt.show()

# LSTM Model :

# Feature Engenering:

# Lag Features
df2['Close_Lag1'] = df2['Close'].shift(1)
df2['Close_Lag2'] = df2['Close'].shift(2)

# Return Features
df2['Close_Return1'] = df2['Close'].pct_change(1)
df2['Close_Return2'] = df2['Close'].pct_change(2)

# Moving Averages
df2['MA10'] = df2['Close'].rolling(window=10).mean()
df2['MA30'] = df2['Close'].rolling(window=30).mean()

 # EMA
df2['EMA_12'] = df2['Close'].ewm(span=12, adjust=False).mean()
df2['EMA_26'] = df2['Close'].ewm(span=26, adjust=False).mean()

 # RSI
delta = df2['Close'].diff()
gain = np.where(delta > 0, delta, 0)
loss = np.where(delta < 0, -delta, 0)

# Flatten the arrays before creating Series
avg_gain = pd.Series(gain.flatten()).rolling(window=14).mean()
avg_loss = pd.Series(loss.flatten()).rolling(window=14).mean()
rs = avg_gain / avg_loss
df2['RSI_14'] = 100 - (100 / (1 + rs))

# MACD
ema_fast = df2['Close'].ewm(span=12, adjust=False).mean()
ema_slow = df2['Close'].ewm(span=26, adjust=False).mean()
df2['MACD'] = ema_fast - ema_slow
df2['MACD_Signal'] = df2['MACD'].ewm(span=9, adjust=False).mean()
df2['MACD_Hist'] = df2['MACD'] - df2['MACD_Signal']

# Bollinger Bands
df2['BB_MA20'] = df2['Close'].rolling(window=20).mean()
std = df2['Close'].rolling(window=20).std()
df2['BB_Upper'] = df2['BB_MA20'] + 2 * std
df2['BB_Lower'] = df2['BB_MA20'] - 2 * std
df2['BB_Width'] = df2['BB_Upper'] - df2['BB_Lower']



# Volume-based
df2['Volume_pct_change'] = df2['Volume'].pct_change()
df2['Volume_MA10'] = df2['Volume'].rolling(window=10).mean()
df2['Volume_STD10'] = df2['Volume'].rolling(window=10).std()

# drop the non numeric columns
df2.drop(columns=['DayOfWeek','RSI_14'], inplace=True)

df2.info()

missing_percent = df2.isnull().mean() * 100
print("Percentage of missing values per column:")
print(missing_percent.sort_values(ascending=False))

# Drop columns with more than 20% missing values for example
cols_to_drop_missing = missing_percent[missing_percent > 20].index.tolist()
print(f"Columns to drop due to missing data >20%: {cols_to_drop_missing}")

# Impute remaining missing values with forward fill (and backward fill as fallback)
df2= df2.fillna(method='ffill').fillna(method='bfill')

# Verify no more missing values remain
print(df2.isnull().sum())

# Plot the corr plot to see tha corr  with each other columns
correlation = df2.corr(numeric_only=True)
# Plot heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Stocks")
plt.tight_layout()
plt.show()

df2.head()

#LSTM

# If your df2 has a 'Date' column (not index), make it the index
if not isinstance(df2.index, pd.DatetimeIndex) and 'Date' in df2.columns:
    df2 = df2.set_index(pd.to_datetime(df2['Date'])).drop(columns=['Date'])

df2 = df2.sort_index()

# Keep only numeric columns
data = df2.select_dtypes(include=np.number).copy()

# Drop NaNs/infs
data.replace([np.inf, -np.inf], np.nan, inplace=True)
data.dropna(inplace=True)

# ===================== 2) Scale Data =====================
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data.values)
close_idx = data.columns.get_loc('Close')

# Helper for inverse scaling Close only
inv_close = lambda x: x * (scaler.data_max_[close_idx] - scaler.data_min_[close_idx]) + scaler.data_min_[close_idx]

# ===================== 3) Create Dataset =====================
time_steps = 60
X, y = [], []
for i in range(time_steps, len(scaled_data)):
    X.append(scaled_data[i-time_steps:i])
    y.append(scaled_data[i, close_idx])
X, y = np.array(X), np.array(y)

# ===================== 4) Train/Test Split =====================
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]
train_idx = data.index[time_steps:train_size+time_steps]
test_idx  = data.index[train_size+time_steps:]

# ===================== 5) Build Model =====================
model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(time_steps, X.shape[2])),
    LSTM(50),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')

# ===================== 6) Train Model =====================
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)

# ===================== 8) Forecast Future =====================
n_days = 10
forecast_in = scaled_data[-time_steps:].reshape(1, time_steps, scaled_data.shape[1])
forecast_scaled = []

for i in range(n_days):
    next_scaled = model.predict(forecast_in)[0, 0]
    forecast_scaled.append(next_scaled)
    last_row = forecast_in[0, -1, :].copy()
    last_row[close_idx] = next_scaled
    forecast_in = np.append(forecast_in[:, 1:, :], last_row.reshape(1, 1, -1), axis=1)

forecast_values = inv_close(np.array(forecast_scaled))
forecast_dates = pd.date_range(start=data.index[-1] + pd.Timedelta(days=1), periods=n_days, freq='B')
forecast_df = pd.DataFrame({'Forecasted_Close': forecast_values}, index=forecast_dates)

# ===================== 7) Predictions on Test =====================
y_pred_test_scaled = model.predict(X_test).flatten()
y_pred_test = inv_close(y_pred_test_scaled)
y_test_inv = inv_close(y_test)

rmse = sqrt(mean_squared_error(y_test_inv, y_pred_test))
print(f"Test RMSE: {rmse:.3f}")

# ===================== 9) ROI Calculations =====================
last_actual_price = data['Close'].iloc[-1]
forecast_df['Daily_ROI_%'] = forecast_df['Forecasted_Close'].pct_change().fillna(0) * 100
forecast_df['Cumulative_ROI_%'] = (forecast_df['Forecasted_Close'] / last_actual_price - 1) * 100

print(forecast_df.round(2))

# =====================  Plot Forecasted Close price  =====================
fig_forecast = go.Figure()
fig_forecast.add_trace(go.Scatter(
    x=forecast_df.index, y=forecast_df['Forecasted_Close'],
    mode='lines+markers', name='Forecast',
    line=dict(color='cyan', width=2)
))
fig_forecast.update_layout(
    title=f'{n_days}-Day Forecasted Close (LSTM)',
    xaxis_title='Date', yaxis_title='Price',
    plot_bgcolor='black', paper_bgcolor='black',
    font=dict(color='white', size=14)
)
fig_forecast.show()

# =====================  Plot Forecasted ROI %  =====================
fig_roi = go.Figure()
fig_roi.add_trace(go.Scatter(
    x=forecast_df.index, y=forecast_df['Cumulative_ROI_%'],
    mode='lines+markers', name='Cumulative ROI (%)',
    line=dict(color='yellow', width=2)
))
fig_roi.update_layout(
    title=f'{n_days}-Day Forecasted Cumulative ROI',
    xaxis_title='Date', yaxis_title='ROI (%)',
    plot_bgcolor='black', paper_bgcolor='black',
    font=dict(color='white', size=14)
)
fig_roi.show()

# =====================  Plot : Train vs Test vs Forecast (LSTM)  =====================

y_train_inv = inv_close(y_train)

fig_combined = go.Figure()
fig_combined.add_trace(go.Scatter(
    x=train_idx, y=y_train_inv, mode='lines', name='Training',
    line=dict(color='deepskyblue', width=2)
))
fig_combined.add_trace(go.Scatter(
    x=test_idx, y=y_test_inv, mode='lines', name='Actual Test',
    line=dict(color='white', width=2)
))
fig_combined.add_trace(go.Scatter(
    x=test_idx, y=y_pred_test, mode='lines', name='Predicted Test',
    line=dict(color='red', dash='dash', width=2)
))
fig_combined.add_trace(go.Scatter(
    x=forecast_df.index, y=forecast_df['Forecasted_Close'],
    mode='lines+markers', name='Forecast',
    line=dict(color='lime', dash='dash', width=2)
))
fig_combined.update_layout(
    title='Close: Train vs Test vs Forecast (LSTM)',
    xaxis_title='Date', yaxis_title='Price',
    plot_bgcolor='black', paper_bgcolor='black',
    font=dict(color='white', size=14)
)
fig_combined.show()



